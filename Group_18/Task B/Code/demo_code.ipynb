{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Task B: BiLSTM Model - Demo\n","This notebook loads the trained model, saved tokenizer file from training, and test data to generate binary predictions (0 or 1), determining whether the given hypothesis is logically entailed by its corresponding premise.\n","\n","This demo performs the following:\n","\n","- Loads and preprocesses the test.csv evaluation dataset.\n","- Loads our trained BiLSTM model.\n","- Runs model.predict() to prediction the labels from the given inputs.\n","- Saves predictions to predictions.csv.\n","\n","*_Note: Please ensure to download trained model and load tokenizer file._*\n"],"metadata":{"id":"6WKSv7EK6kMe"}},{"cell_type":"markdown","source":["# Installing Required Packages"],"metadata":{"id":"4Wy1MbR80peR"}},{"cell_type":"code","source":["!pip install nltk"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vw_uZ7Xlt6xD","outputId":"9696f649-4dc4-4244-d0e8-c1fbad87d765"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"]}]},{"cell_type":"markdown","source":["# Data Loading, Model Loading and NLTK Setup\n","\n","Loading in saved model, loading in test dataset and downloading neccesary nltk packages for preprocessing."],"metadata":{"id":"5fh5i6XZ0tGm"}},{"cell_type":"code","source":["import pandas as pd\n","import tensorflow as tf\n","import nltk\n","from tensorflow.keras.models import load_model\n","import tensorflow.keras.backend as K\n","nltk.download('punkt_tab')\n","nltk.download('wordnet')\n","\n","def abs_diff(x):\n","    return K.abs(x)\n","\n","model = load_model(\"best_nli_model_B.keras\", custom_objects={'abs_diff': abs_diff})\n","df_test = pd.read_csv(\"/content/test.csv\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g3Rj_KVv5Ss3","outputId":"e034f841-6ebd-489a-d249-99cd704c244b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]}]},{"cell_type":"markdown","source":["# Text Preprocessing for Premises and Hypotheses\n","\n","This step cleans the text data for both the premise and hypothesis columns in the dataset.\n","\n","## Preprocessing Pipeline Includes:\n","- Lowercasing the text\n","- Removing non-alphabetic characters\n","- Tokenising using NLTK's word_tokenize\n","- Lemmatising using WordNetLemmatizer"],"metadata":{"id":"buZoxw6m073W"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"8ROgC1Ql4nmf"},"outputs":[],"source":["from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import stopwords\n","import numpy as np\n","import re\n","\n","\n","lemmatizer = WordNetLemmatizer()\n","\n","#Preprocess the data\n","def preprocessing(texts):\n","    preprocessed_texts = []\n","\n","    for text in texts:\n","        if not text:  #handle empty text\n","            continue\n","\n","        #clean the text: lowercase, strip whitespace, and remove non-alphabetic characters\n","        cleaned_text = re.sub(r'[^A-Za-z\\s]', '', text.lower().strip())\n","\n","        #tokenise the cleaned text\n","        tokenized_words = [word for word in word_tokenize(cleaned_text)]\n","\n","        lemmatized_words = [lemmatizer.lemmatize(word) for word in tokenized_words]\n","\n","        #append the lemmatised words to the list\n","        preprocessed_texts.append(lemmatized_words)\n","\n","    return preprocessed_texts\n","\n","\n","#apply preprocessing\n","premise_test = preprocessing(df_test['premise'])\n","hypothesis_test = preprocessing(df_test['hypothesis'])\n"]},{"cell_type":"markdown","source":["# Prepare Test Data\n","In this step, the preprocessed premise and hypothesis texts are converted into padded sequences of integers.\n","\n","## What This Step Does:\n","###  Tokenisation\n","- We load the tokeniser file obtained from training the model. This is to ensure consistency and guarantees that words are mapped to the same indices.\n","- The tokenised texts are then converted into sequences of integers.\n","\n","### Padding Sequences\n","- During training, we calculated the maximum sequence length (max_len) across the training data to standardise input shapes for the model. This value must remain consistent during evaluation to ensure that the input shapes match the model's expected dimensions.\n","-We then use pad_sequences to pad each sequence to this length using 'post' padding."],"metadata":{"id":"8UwVwLuV1D3r"}},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import pickle\n","\n","#load the tokenizer\n","with open(\"tokenizer.pkl\", \"rb\") as f:\n","    tokenizer = pickle.load(f)\n","\n","\n","max_len = 281\n","#convert to sequences using your existing tokenizer\n","premise_test_seq = tokenizer.texts_to_sequences(premise_test)\n","hypothesis_test_seq = tokenizer.texts_to_sequences(hypothesis_test)\n","\n","#Pad sequences\n","premise_test_pad = tf.keras.preprocessing.sequence.pad_sequences(premise_test_seq, maxlen=max_len, padding='post')\n","hypothesis_test_pad = tf.keras.preprocessing.sequence.pad_sequences(hypothesis_test_seq, maxlen=max_len, padding='post')"],"metadata":{"id":"RWRbNR4CNp2I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Final Prediction on Test Data\n","We use the saved and loaded model to generate predictions on the unseen test data.\n","\n","1. Predict Probabilities: We use the trained model to predict the probabilities of entailment for each input pair in the test data.\n","2. Apply Optimal Threshold: We apply the best threshold (0.489) determined from the development set based on the highest Matthews Correlation Coefficient (MCC).\n","3. Save Predictions: The binary predictions are saved to a CSV file named Group_18_B.csv."],"metadata":{"id":"hGBCBSte1LyO"}},{"cell_type":"code","source":["#predict probabilities\n","preds_test = model.predict([premise_test_pad, hypothesis_test_pad])\n","best_thresh = 0.48999999999999977 #from tuning and dev evaluation\n","\n","#apply threshold\n","pred_labels_test = (preds_test > best_thresh).astype(int) #best threshold from evaluation\n","\n","#save to CSV\n","prediction_df = pd.DataFrame({'prediction': pred_labels_test.flatten()})\n","prediction_df.to_csv(\"Group_18_B.csv\", index=False)"],"metadata":{"id":"Is_Q32Q85KRE","colab":{"base_uri":"https://localhost:8080/"},"outputId":"416cb7b2-71f0-4553-9265-e5422f8671ea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 730ms/step\n"]}]}]}