{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11278336,"sourceType":"datasetVersion","datasetId":7051074}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[]}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"b16f7a30","cell_type":"markdown","source":["## Hyperparameter Tuning for DeBERTa + BiLSTM using Optuna\n","\n","This notebook performs hyperparameter tuning for a hybrid **DeBERTa + BiLSTM model** on a binary Natural Language Inference (NLI) task using **Optuna**.\n","\n","The goal is to find the optimal values for:\n","\n","- Learning rate\n","- Weight decay\n","- Dropout rate\n","- Hidden dimension of the LSTM\n","- Batch size\n","- Number of training epochs\n","\n","We use validation accuracy as the optimization metric, and Optuna's `MedianPruner` to speed up the tuning process by pruning underperforming trials early.\n"],"metadata":{"id":"b16f7a30"}},{"cell_type":"code","source":["import torch\n","import pandas as pd\n","import numpy as np\n","import optuna\n","import matplotlib.pyplot as plt\n","from torch.utils.data import DataLoader, Dataset\n","import torch.nn as nn\n","from torch.nn.functional import softmax\n","from sklearn.metrics import accuracy_score\n","from transformers import AutoModel, AutoTokenizer\n","from optuna.trial import TrialState\n","\n","# Set device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"tEP6iouKW5uO"},"id":"tEP6iouKW5uO","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Load Tokenizer and Dataset\n","\n","We load the pretrained tokenizer (`microsoft/deberta-v3-base`) and read the `train.csv` and `dev.csv` datasets, which contain `premise`, `hypothesis`, and `label` columns.\n"],"metadata":{"id":"_c2-Dc3oW7Zb"},"id":"_c2-Dc3oW7Zb"},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n","df_train = pd.read_csv('/kaggle/input/training/train.csv')\n","df_val = pd.read_csv('/kaggle/input/training/dev.csv')"],"metadata":{"id":"FOhbAO42W8w5"},"id":"FOhbAO42W8w5","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Define Dataset Class and Tokenization Function\n","\n","We define:\n","- A `NLIDataset` class that prepares batches of tokenized input.\n","- A helper function to tokenize `premise` and `hypothesis` pairs using a max length of 128.\n"],"metadata":{"id":"Wc8moJ0vXCHX"},"id":"Wc8moJ0vXCHX"},{"cell_type":"code","source":["# Dataset class\n","class NLIDataset(Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        return {\n","            'input_ids': self.encodings['input_ids'][idx],\n","            'attention_mask': self.encodings['attention_mask'][idx],\n","            'labels': self.labels[idx]\n","        }\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","# Tokenization function\n","def tokenize_premise_hypothesis(premises, hypotheses, max_length=128):\n","    return tokenizer(\n","        premises,\n","        hypotheses,\n","        padding='max_length',\n","        truncation=True,\n","        max_length=max_length,\n","        return_tensors='pt'\n","    )"],"metadata":{"id":"Suy4v9boXLiA"},"id":"Suy4v9boXLiA","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Define the DeBERTa + BiLSTM Model\n","\n","The model combines:\n","- A DeBERTa-v3 transformer encoder\n","- A bidirectional LSTM (`hidden_dim`, 1 layer)\n","- Dropout\n","- A fully connected classification layer (`Linear(hidden_dim * 2, 2)`)\n"],"metadata":{"id":"Xc9oZeP4XMYv"},"id":"Xc9oZeP4XMYv"},{"cell_type":"code","source":["# BiLSTM model\n","class DeBERTaWithBiLSTM(nn.Module):\n","    def __init__(self, hidden_dim=256, dropout=0.3):\n","        super().__init__()\n","        self.base_model = AutoModel.from_pretrained(\"microsoft/deberta-v3-base\")\n","        self.bilstm = nn.LSTM(768, hidden_dim, num_layers=1, bidirectional=True, batch_first=True)\n","        self.dropout = nn.Dropout(dropout)\n","        self.classifier = nn.Linear(hidden_dim * 2, 2)\n","\n","    def forward(self, input_ids, attention_mask, labels=None):\n","        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n","        sequence_output = outputs.last_hidden_state\n","        lstm_out, _ = self.bilstm(sequence_output)\n","        pooled_output = lstm_out[:, 0]\n","        out = self.dropout(pooled_output)\n","        logits = self.classifier(out)\n","        if labels is not None:\n","            loss_fn = nn.CrossEntropyLoss()\n","            loss = loss_fn(logits, labels)\n","            return {'loss': loss, 'logits': logits}\n","        return {'logits': logits}"],"metadata":{"id":"SpiVMUEDXSyK"},"id":"SpiVMUEDXSyK","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Define Optuna Objective Function\n","\n","This function:\n","- Samples hyperparameters from given ranges.\n","- Initializes the model and optimizer.\n","- Trains the model for `num_epochs`.\n","- Evaluates on the validation set each epoch.\n","- Uses `trial.report()` and `trial.should_prune()` for pruning.\n","\n","Validation accuracy is returned as the optimization metric.\n","\n","We use `MedianPruner` to prune underperforming trials early. The study is configured to maximize validation accuracy.\n","\n","We run 20 trials to explore different hyperparameter combinations."],"metadata":{"id":"jXaaPwBKXTaf"},"id":"jXaaPwBKXTaf"},{"id":"93d302c2","cell_type":"code","source":["\n","\n","# Optuna objective\n","def objective(trial):\n","    learning_rate = trial.suggest_float(\"learning_rate\", 5e-6, 3e-5, log=True)\n","    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 5e-4, log=True)\n","    dropout = trial.suggest_float(\"dropout\", 0.2, 0.4)\n","    hidden_dim = trial.suggest_categorical(\"hidden_dim\", [128, 256, 384])\n","    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32])\n","    num_epochs = trial.suggest_int(\"num_epochs\", 5, 10)\n","\n","    train_enc = tokenize_premise_hypothesis(df_train['premise'].tolist(), df_train['hypothesis'].tolist())\n","    val_enc = tokenize_premise_hypothesis(df_val['premise'].tolist(), df_val['hypothesis'].tolist())\n","    train_labels = torch.tensor(df_train['label'].values)\n","    val_labels = torch.tensor(df_val['label'].values)\n","\n","    train_dataset = NLIDataset(train_enc, train_labels)\n","    val_dataset = NLIDataset(val_enc, val_labels)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n","\n","    model = DeBERTaWithBiLSTM(hidden_dim=hidden_dim, dropout=dropout).to(device)\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        for batch in train_loader:\n","            optimizer.zero_grad()\n","            outputs = model(\n","                input_ids=batch['input_ids'].to(device),\n","                attention_mask=batch['attention_mask'].to(device),\n","                labels=batch['labels'].to(device)\n","            )\n","            loss = outputs['loss']\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","            optimizer.step()\n","\n","        # Evaluate after each epoch for pruning\n","        model.eval()\n","        all_preds, all_labels = [], []\n","        with torch.no_grad():\n","            for batch in val_loader:\n","                outputs = model(\n","                    input_ids=batch['input_ids'].to(device),\n","                    attention_mask=batch['attention_mask'].to(device),\n","                    labels=batch['labels'].to(device)\n","                )\n","                preds = torch.argmax(outputs['logits'], dim=1)\n","                all_preds.extend(preds.cpu().numpy())\n","                all_labels.extend(batch['labels'].cpu().numpy())\n","\n","        val_acc = accuracy_score(all_labels, all_preds)\n","        trial.report(val_acc, epoch)\n","\n","        if trial.should_prune():\n","            raise optuna.exceptions.TrialPruned()\n","\n","    return val_acc\n","\n","# Run Optuna with pruning\n","pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=1)\n","study = optuna.create_study(direction=\"maximize\", pruner=pruner)\n","study.optimize(objective, n_trials=20)\n","\n","# Print best trial\n","print(\"✅ Best Hyperparameters Found:\")\n","for key, value in study.best_trial.params.items():\n","    print(f\"{key}: {value}\")\n"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:58:43.383718Z","iopub.execute_input":"2025-04-07T01:58:43.384007Z","iopub.status.idle":"2025-04-07T11:33:59.004452Z","shell.execute_reply.started":"2025-04-07T01:58:43.383976Z","shell.execute_reply":"2025-04-07T11:33:59.003548Z"},"colab":{"referenced_widgets":["2740fe69798e471e9275ce2c5841ba83","49f6e48239b9485281314d7feb18e624","f22a2140522f49f0b062f28e6eecf289","41503677660045539b1dd30be96ab946"]},"id":"93d302c2","outputId":"26040a14-fd6b-4b60-b78c-a09e5adc5936"},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2740fe69798e471e9275ce2c5841ba83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49f6e48239b9485281314d7feb18e624"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f22a2140522f49f0b062f28e6eecf289"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n[I 2025-04-07 01:58:54,505] A new study created in memory with name: no-name-e05d654e-c658-4754-91f9-2d9e1a70e844\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41503677660045539b1dd30be96ab946"}},"metadata":{}},{"name":"stderr","text":"[I 2025-04-07 03:07:38,190] Trial 0 finished with value: 0.9073634204275535 and parameters: {'learning_rate': 1.1596292509631827e-05, 'weight_decay': 1.86556109418857e-05, 'dropout': 0.2629848916241302, 'hidden_dim': 256, 'batch_size': 32, 'num_epochs': 10}. Best is trial 0 with value: 0.9073634204275535.\n[I 2025-04-07 04:24:25,228] Trial 1 finished with value: 0.9125593824228029 and parameters: {'learning_rate': 1.5191865828425241e-05, 'weight_decay': 0.00029798996534771856, 'dropout': 0.2127259462963618, 'hidden_dim': 256, 'batch_size': 16, 'num_epochs': 10}. Best is trial 1 with value: 0.9125593824228029.\n[I 2025-04-07 05:10:32,887] Trial 2 finished with value: 0.9103325415676959 and parameters: {'learning_rate': 2.1124594298151137e-05, 'weight_decay': 7.028390337983515e-06, 'dropout': 0.3275037533574017, 'hidden_dim': 256, 'batch_size': 16, 'num_epochs': 6}. Best is trial 1 with value: 0.9125593824228029.\n[I 2025-04-07 06:20:15,132] Trial 3 finished with value: 0.9171615201900237 and parameters: {'learning_rate': 6.4489491236745285e-06, 'weight_decay': 8.443071487473449e-05, 'dropout': 0.29351324417991814, 'hidden_dim': 384, 'batch_size': 32, 'num_epochs': 10}. Best is trial 3 with value: 0.9171615201900237.\n[I 2025-04-07 07:00:09,397] Trial 4 finished with value: 0.9171615201900237 and parameters: {'learning_rate': 2.1465227780127935e-05, 'weight_decay': 8.168485421986985e-05, 'dropout': 0.21791908614748035, 'hidden_dim': 128, 'batch_size': 32, 'num_epochs': 6}. Best is trial 3 with value: 0.9171615201900237.\n[I 2025-04-07 07:14:10,234] Trial 5 pruned. \n[I 2025-04-07 07:29:05,085] Trial 6 pruned. \n[I 2025-04-07 07:44:54,168] Trial 7 pruned. \n[I 2025-04-07 08:00:20,241] Trial 8 pruned. \n[I 2025-04-07 08:14:21,773] Trial 9 pruned. \n[I 2025-04-07 08:49:17,186] Trial 10 finished with value: 0.9202790973871734 and parameters: {'learning_rate': 8.663394579529044e-06, 'weight_decay': 0.00043725068136265345, 'dropout': 0.38920213235632034, 'hidden_dim': 384, 'batch_size': 32, 'num_epochs': 5}. Best is trial 10 with value: 0.9202790973871734.\n[I 2025-04-07 09:03:18,663] Trial 11 pruned. \n[I 2025-04-07 09:17:20,569] Trial 12 pruned. \n[I 2025-04-07 09:31:22,416] Trial 13 pruned. \n[I 2025-04-07 09:45:24,124] Trial 14 pruned. \n[I 2025-04-07 09:59:25,717] Trial 15 pruned. \n[I 2025-04-07 10:12:46,878] Trial 16 pruned. \n[I 2025-04-07 10:26:50,444] Trial 17 pruned. \n[I 2025-04-07 10:40:51,386] Trial 18 pruned. \n[I 2025-04-07 11:33:58,995] Trial 19 finished with value: 0.9134501187648456 and parameters: {'learning_rate': 1.514412615337396e-05, 'weight_decay': 0.00011865365217406775, 'dropout': 0.321508186009621, 'hidden_dim': 128, 'batch_size': 32, 'num_epochs': 8}. Best is trial 10 with value: 0.9202790973871734.\n","output_type":"stream"},{"name":"stdout","text":"✅ Best Hyperparameters Found:\nlearning_rate: 8.663394579529044e-06\nweight_decay: 0.00043725068136265345\ndropout: 0.38920213235632034\nhidden_dim: 384\nbatch_size: 32\nnum_epochs: 5\n","output_type":"stream"}],"execution_count":null}]}